# Mixtral_on_24Gb_VRAM
This is a simple exemple of running Mixtral 8x7b the Mixture of expert 

from Mistral AI or it's derivatives on CUDA with only 24Gb of Video RAM 

to ensure it works on a single Consumer GPU (GTX 3090 or RTX 4090)

It is not fast (around 1 token/second) but it is simple and it works
